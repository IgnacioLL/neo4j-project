{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92965c76-3af7-4627-8d5d-1f3bc7a4d1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType, DateType, LongType, DoubleType\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit, when, col\n",
    "import pandas as pd\n",
    "from pyspark.sql import Window\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fa07e6f-ad5e-48e6-a6a6-adf7c45a29d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/04/09 19:40:04 WARN Utils: Your hostname, MyLaptop resolves to a loopback address: 127.0.1.1; using 172.28.44.164 instead (on interface eth0)\n",
      "24/04/09 19:40:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/09 19:40:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder \n",
    "    .appName(\"Preprocessing SDM\") \n",
    "    .config(\"spark.driver.memory\", \"1g\") \n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a42dc931-2034-472f-b0a7-650d4c15b0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory =  \"/mnt/c/MDS/Q2/SDM/data/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5bb106-fa48-4e36-b913-b67513801c4a",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d53d95-7b1f-41eb-a2b8-83e7dfd512eb",
   "metadata": {},
   "source": [
    "#### Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef6acb3f-13e2-410d-a2f1-3c7e87eca28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_schema(headers):\n",
    "    schema = []\n",
    "    for columna, tipo in headers:\n",
    "        if tipo in ['int', 'ID']: spark_dtype = LongType()\n",
    "        elif str(tipo).startswith(\"string\"): spark_dtype = StringType()\n",
    "        else: spark_dtype = StringType()\n",
    "\n",
    "        if columna == '': columna = tipo\n",
    "    \n",
    "        schema.append(StructField(columna, spark_dtype, True))\n",
    "    return StructType(schema)\n",
    "\n",
    "\n",
    "def read_data_w_sep_headers(name_data: str = \"dblp_www\"):\n",
    "    headers = pd.read_csv(f\"{root_directory}/{name_data}_header.csv\", delimiter=\";\").columns\n",
    "    headers = map(lambda x: str(x).split(\":\"), headers)\n",
    "    \n",
    "    schema = _create_schema(headers)\n",
    "\n",
    "    df = spark.read.schema(schema).options(delimiter=\";\").csv(f\"{root_directory}/{name_data}.csv\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def read_data_w_embedded_headers(name_data: str = \"dblp_www\"):\n",
    "    headers = pd.read_csv(f\"{root_directory}/{name_data}.csv\", delimiter=\";\",nrows=1).columns\n",
    "    headers = map(lambda x: str(x).split(\":\"), headers)\n",
    "    \n",
    "    schema = _create_schema(headers)\n",
    "\n",
    "    df = spark.read.schema(schema).options(delimiter=\";\", header=\"true\").csv(f\"{root_directory}/{name_data}.csv\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_fake_email(df: pyspark.sql.DataFrame):\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\"cleaned_name\", F.regexp_replace(F.lower(\"name\"), \"[^a-zA-Z0-9\\s]\", \"\"))\n",
    "        .withColumn(\"email\", F.concat(F.regexp_replace(\"cleaned_name\", \" \", \".\"), lit(\"@gmail.com\")))\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e04cdabd-cfa0-4ddc-8b14-5be658408fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = read_data_w_sep_headers(name_data=\"dblp_article\")\n",
    "book = read_data_w_sep_headers(name_data=\"dblp_book\")\n",
    "data = read_data_w_sep_headers(name_data=\"dblp_data\")\n",
    "incollection = read_data_w_sep_headers(name_data=\"dblp_incollection\")\n",
    "inproceedings = read_data_w_sep_headers(name_data=\"dblp_inproceedings\")\n",
    "mastersthesis = read_data_w_sep_headers(name_data=\"dblp_mastersthesis\")\n",
    "phdthesis = read_data_w_sep_headers(name_data=\"dblp_phdthesis\")\n",
    "proceedings = read_data_w_sep_headers(name_data=\"dblp_proceedings\")\n",
    "www = read_data_w_sep_headers(name_data=\"dblp_www\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20209075-8b0f-458e-9276-8c68d8efa438",
   "metadata": {},
   "outputs": [],
   "source": [
    "author = read_data_w_embedded_headers(name_data=\"dblp_author\")\n",
    "authored_by = read_data_w_embedded_headers(name_data=\"dblp_author_authored_by\")\n",
    "journal = read_data_w_embedded_headers(name_data=\"dblp_journal\")\n",
    "journal_published_in = read_data_w_embedded_headers(name_data=\"dblp_journal_published_in\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f3fa24e-4dc7-49cf-bad5-fad30d36531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = [article, book, data, incollection, inproceedings, mastersthesis, phdthesis, proceedings, www]\n",
    "name_tables = [\"article\", \"book\", \"data\", \"incollection\", \"inproceedings\", \"mastersthesis\", \"phdthesis\", \"proceedings\", \"www\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06468ef8",
   "metadata": {},
   "source": [
    "#### Creation of authors data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66ad190b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "authors = article.withColumn(\"name\", F.explode(F.split(\"author\", \"\\|\"))).select(\"name\").distinct()\n",
    "authors_data = create_fake_email(df=authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53b0efb",
   "metadata": {},
   "source": [
    "#### Creation of article data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5b359e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data = article.select(\n",
    "    col(\"article\").alias(\"id\"),\n",
    "    col(\"author\").alias(\"author_name\"),\n",
    "    \"journal\",\n",
    "    \"title\", \n",
    "    \"url\",\n",
    "    \"volume\",\n",
    "    \"year\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27609cc0",
   "metadata": {},
   "source": [
    "#### Creation of journal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8563223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_data = article.select(\n",
    "    \"journal\"\n",
    ").dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2024d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window().orderBy(\"n_articles\")\n",
    "journal_rank = (\n",
    "    article\n",
    "    .groupBy(\"journal\")\n",
    "    .agg(F.countDistinct(\"article\").alias(\"n_articles\"))\n",
    "    .orderBy(col(\"n_articles\").desc())\n",
    "    .limit(200)\n",
    "    .withColumn(\"rank\", F.row_number().over(w))\n",
    "    .select(\"rank\", \"journal\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a183c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_data_w_rank = journal_data.join(journal_rank, on=\"journal\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cd6508",
   "metadata": {},
   "source": [
    "#### Creation of time data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8d87adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = article.select(\"year\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7142e8",
   "metadata": {},
   "source": [
    "### Creation of conferences information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66ad190b",
   "metadata": {},
   "outputs": [],
   "source": [
    "proceedings_ = (\n",
    "    proceedings\n",
    "    .withColumn(\"type\", F.split(\"url\", \"\\/\")[1])\n",
    "    .withColumn(\"conference_name\", F.split(\"url\", \"\\/\")[2])\n",
    "    .withColumn(\"edition\", F.concat_ws(\"-\",\"conference_name\", \"year\"))\n",
    "    .filter(col(\"type\") == 'conf')\n",
    "    .select(\"type\",\"conference_name\",\"edition\", \"editor\", \"year\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0f67986",
   "metadata": {},
   "outputs": [],
   "source": [
    "inproceedings_ = (\n",
    "    inproceedings\n",
    "    .withColumn(\"type\", F.split(\"url\", \"\\/\")[1])\n",
    "    .withColumn(\"conference_name\", F.split(\"url\", \"\\/\")[2])\n",
    "    .withColumn(\"edition\", F.concat_ws(\"-\",\"conference_name\", \"year\"))\n",
    "    .filter(col(\"type\") == 'conf')\n",
    "    .select(\"type\",\"conference_name\",\"edition\", \"editor\", \"year\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "672aa58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conference_information = proceedings_.union(inproceedings_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cf6813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = [ \n",
    "    \"New York City, USA\", \"London, UK\", \"Tokyo, Japan\", \"Paris, France\", \"Los Angeles, USA\", \"Beijing, China\", \"Moscow, Russia\", \"Istanbul, Turkey\", \"Sao Paulo, Brazil\",\n",
    "    \"Cairo, Egypt\", \"Mumbai, India\", \"Mexico City, Mexico\", \"Seoul, South Korea\", \"Jakarta, Indonesia\", \"Karachi, Pakistan\", \"Buenos Aires, Argentina\", \"Delhi, India\", \n",
    "    \"Shanghai, China\", \"Manila, Philippines\", \"Dhaka, Bangladesh\", \"Moscow, Russia\", \"Istanbul, Turkey\", \"Tianjin, China\", \"Rio de Janeiro, Brazil\", \"Lagos, Nigeria\", \"Lima, Peru\", \n",
    "    \"Bangkok, Thailand\", \"Jakarta, Indonesia\", \"Cairo, Egypt\", \"Bogota, Colombia\", \"Kinshasa, Democratic Republic of the Congo\", \"Seoul, South Korea\", \"Dhaka, Bangladesh\", \"Karachi, Pakistan\", \n",
    "    \"Tokyo, Japan\", \"Manila, Philippines\", \"Guangzhou, China\", \"Mumbai, India\", \"Istanbul, Turkey\", \"Moscow, Russia\", \"Sao Paulo, Brazil\", \"Beijing, China\", \"Lahore, Pakistan\", \n",
    "    \"Shenzhen, China\", \"Chongqing, China\", \"Chengdu, China\", \"Lahore, Pakistan\", \"Kinshasa, Democratic Republic of the Congo\", \"Bangalore, India\", \"Taipei, Taiwan\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f088809",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.orderBy(\"city\")\n",
    "cities = spark.createDataFrame([(city,) for city in cities], [\"city\"]).withColumn(\"number\", F.row_number().over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3288611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conference_information_w_random_city = (\n",
    "    conference_information\n",
    "    .withColumn(\"number\", F.floor(F.rand()*50))\n",
    "    .join(F.broadcast(cities), on=\"number\", how=\"left\")\n",
    "    .dropDuplicates(['edition'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca3edb6",
   "metadata": {},
   "source": [
    "#### Create conference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5deb6483",
   "metadata": {},
   "outputs": [],
   "source": [
    "conference_data = conference_information_w_random_city.select(\"conference_name\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7395d8c",
   "metadata": {},
   "source": [
    "#### Create edition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39e45bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "edition_data = conference_information_w_random_city.select(\"conference_name\", col(\"edition\").alias(\"conference_edition\"), \"city\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf06ec2",
   "metadata": {},
   "source": [
    "#### Create city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49c0a3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_data = cities.drop(\"number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7383fb2",
   "metadata": {},
   "source": [
    "#### Create random link between article data and conference edition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26778775",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_info_random = conference_information_w_random_city.withColumn(\"number\", F.round(F.rand(), 3)).select(\"edition\",\"year\", \"number\")\n",
    "article_data_w_conf_info = article_data.withColumn(\"number\", F.round(F.rand(), 3)).join(conf_info_random, on=[\"number\",\"year\"], how=\"left\").drop(\"number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60b0c29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewers_data = conference_information_w_random_city.select(F.explode(F.split(\"editor\", \"\\|\")).alias(\"reviewer\"), \"edition\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf216cd",
   "metadata": {},
   "source": [
    "#### Create random link between reviewers and articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b962f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_reviewer_article = (\n",
    "    article_data_w_conf_info\n",
    "    .withColumn(\"number\", F.round(F.rand()/2,1)).select(\"ID\",\"edition\", \"number\")\n",
    "    .join(reviewers_data.withColumn(\"number\", F.round(F.rand()/2,1)), on=[\"edition\",\"number\"], how=\"left\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e660de19",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_reviewer_article = link_reviewer_article.select(\"ID\",\"reviewer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9142e5",
   "metadata": {},
   "source": [
    "### Persist to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "162b9b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/09 19:32:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/09 19:32:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/09 19:32:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/09 19:32:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/09 19:32:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/09 19:32:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/09 19:32:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/09 19:32:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/09 19:32:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/09 19:32:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/09 19:32:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/09 19:32:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/09 19:32:41 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "24/04/09 19:32:41 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "24/04/09 19:32:41 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "24/04/09 19:32:41 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "24/04/09 19:32:41 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "24/04/09 19:32:46 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "24/04/09 19:32:46 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "24/04/09 19:32:46 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "24/04/09 19:32:46 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "24/04/09 19:33:10 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "24/04/09 19:33:10 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "24/04/09 19:33:10 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "24/04/09 19:33:10 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "24/04/09 19:33:10 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "24/04/09 19:33:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "24/04/09 19:33:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "24/04/09 19:33:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "24/04/09 19:33:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "24/04/09 19:33:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/09 19:33:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/09 19:33:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/09 19:33:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/09 19:33:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/09 19:33:48 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/09 19:33:48 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/04/09 19:33:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/09 19:33:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/09 19:33:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/09 19:33:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/09 19:33:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/09 19:34:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/09 19:34:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/09 19:34:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/09 19:34:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/09 19:35:06 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "24/04/09 19:35:06 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "24/04/09 19:35:06 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "24/04/09 19:35:06 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "24/04/09 19:35:06 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "24/04/09 19:35:08 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "24/04/09 19:35:09 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "24/04/09 19:35:09 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "24/04/09 19:35:09 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "authors_data.write.mode(\"overwrite\").parquet(f\"../temporal_zone/authors_data\")\n",
    "article_data_w_conf_info.write.mode(\"overwrite\").parquet(f\"../temporal_zone/article_data\")\n",
    "journal_data_w_rank.write.mode(\"overwrite\").parquet(f\"../temporal_zone/journal_data_w_rank\")\n",
    "time.write.mode(\"overwrite\").parquet(f\"../temporal_zone/time\")\n",
    "conference_data.write.mode(\"overwrite\").parquet(f\"../temporal_zone/conference_data\")\n",
    "edition_data.write.mode(\"overwrite\").parquet(f\"../temporal_zone/edition_data\")\n",
    "city_data.write.mode(\"overwrite\").parquet(f\"../temporal_zone/city_data\")\n",
    "reviewers_data.write.mode(\"overwrite\").parquet(f\"../temporal_zone/reviewers_data\")\n",
    "link_reviewer_article.write.mode(\"overwrite\").parquet(f\"../temporal_zone/link_reviewer_article\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a6f8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Define a list of file paths\n",
    "file_paths = [\n",
    "    \"../temporal_zone/authors_data\",\n",
    "    \"../temporal_zone/article_data\",\n",
    "    \"../temporal_zone/journal_data_w_rank\",\n",
    "    \"../temporal_zone/time\",\n",
    "    \"../temporal_zone/conference_data\",\n",
    "    \"../temporal_zone/edition_data\",\n",
    "    \"../temporal_zone/city_data\",\n",
    "    \"../temporal_zone/reviewers_data\",\n",
    "    \"../temporal_zone/link_reviewer_article\"\n",
    "]\n",
    "\n",
    "# Loop through each file path\n",
    "for file_path in file_paths:\n",
    "    # Write the DataFrame to CSV\n",
    "    spark.read.parquet(file_path).coalesce(1).write.mode(\"overwrite\").csv(f\"{file_path}_tmp\")\n",
    "\n",
    "    # Get the path of the single CSV file\n",
    "    csv_file_path = f\"{file_path}_tmp/*.csv\"\n",
    "    files = os.listdir(f\"{file_path}_tmp\")\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            csv_file_path = os.path.join(f\"{file_path}_tmp\", file)\n",
    "            break\n",
    "\n",
    "    # Rename the CSV file\n",
    "    os.rename(csv_file_path, f\"{file_path}.csv\")\n",
    "\n",
    "    # Delete the temporary folder\n",
    "    shutil.rmtree(f\"{file_path}_tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7d3468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
