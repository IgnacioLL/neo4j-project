{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92965c76-3af7-4627-8d5d-1f3bc7a4d1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType, DateType, LongType, DoubleType\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit, when, col\n",
    "import pandas as pd\n",
    "from pyspark.sql import Window\n",
    "\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fa07e6f-ad5e-48e6-a6a6-adf7c45a29d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/04/10 21:22:47 WARN Utils: Your hostname, MyLaptop resolves to a loopback address: 127.0.1.1; using 172.28.44.164 instead (on interface eth0)\n",
      "24/04/10 21:22:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/10 21:22:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder \n",
    "    .appName(\"Preprocessing SDM\") \n",
    "    .config(\"spark.driver.memory\", \"2g\") \n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a42dc931-2034-472f-b0a7-650d4c15b0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory =  \"/mnt/c/MDS/Q2/SDM/data/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5bb106-fa48-4e36-b913-b67513801c4a",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d53d95-7b1f-41eb-a2b8-83e7dfd512eb",
   "metadata": {},
   "source": [
    "#### Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef6acb3f-13e2-410d-a2f1-3c7e87eca28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_schema(headers):\n",
    "    schema = []\n",
    "    for columna, tipo in headers:\n",
    "        if tipo in ['int', 'ID']: spark_dtype = LongType()\n",
    "        elif str(tipo).startswith(\"string\"): spark_dtype = StringType()\n",
    "        else: spark_dtype = StringType()\n",
    "\n",
    "        if columna == '': columna = tipo\n",
    "    \n",
    "        schema.append(StructField(columna, spark_dtype, True))\n",
    "    return StructType(schema)\n",
    "\n",
    "\n",
    "def read_data_w_sep_headers(name_data: str = \"dblp_www\"):\n",
    "    headers = pd.read_csv(f\"{root_directory}/{name_data}_header.csv\", delimiter=\";\").columns\n",
    "    headers = map(lambda x: str(x).split(\":\"), headers)\n",
    "    \n",
    "    schema = _create_schema(headers)\n",
    "\n",
    "    df = spark.read.schema(schema).options(delimiter=\";\").csv(f\"{root_directory}/{name_data}.csv\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def read_data_w_embedded_headers(name_data: str = \"dblp_www\"):\n",
    "    headers = pd.read_csv(f\"{root_directory}/{name_data}.csv\", delimiter=\";\",nrows=1).columns\n",
    "    headers = map(lambda x: str(x).split(\":\"), headers)\n",
    "    \n",
    "    schema = _create_schema(headers)\n",
    "\n",
    "    df = spark.read.schema(schema).options(delimiter=\";\", header=\"true\").csv(f\"{root_directory}/{name_data}.csv\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_fake_email(df: pyspark.sql.DataFrame):\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\"cleaned_name\", F.regexp_replace(F.lower(\"name\"), \"[^a-zA-Z0-9\\s]\", \"\"))\n",
    "        .withColumn(\"email\", F.concat(F.regexp_replace(\"cleaned_name\", \" \", \".\"), lit(\"@gmail.com\")))\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e04cdabd-cfa0-4ddc-8b14-5be658408fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = read_data_w_sep_headers(name_data=\"dblp_article\")\n",
    "book = read_data_w_sep_headers(name_data=\"dblp_book\")\n",
    "data = read_data_w_sep_headers(name_data=\"dblp_data\")\n",
    "incollection = read_data_w_sep_headers(name_data=\"dblp_incollection\")\n",
    "inproceedings = read_data_w_sep_headers(name_data=\"dblp_inproceedings\")\n",
    "mastersthesis = read_data_w_sep_headers(name_data=\"dblp_mastersthesis\")\n",
    "phdthesis = read_data_w_sep_headers(name_data=\"dblp_phdthesis\")\n",
    "proceedings = read_data_w_sep_headers(name_data=\"dblp_proceedings\")\n",
    "www = read_data_w_sep_headers(name_data=\"dblp_www\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6ea3e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "prop_data = 5_000/article.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4fbb297",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = article.filter(F.rand() < prop_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd9cb481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# article.write.mode(\"overwrite\").parquet(\"checkpoint3\")\n",
    "article = spark.read.parquet(\"checkpoint3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20209075-8b0f-458e-9276-8c68d8efa438",
   "metadata": {},
   "outputs": [],
   "source": [
    "author = read_data_w_embedded_headers(name_data=\"dblp_author\")\n",
    "authored_by = read_data_w_embedded_headers(name_data=\"dblp_author_authored_by\")\n",
    "journal = read_data_w_embedded_headers(name_data=\"dblp_journal\")\n",
    "journal_published_in = read_data_w_embedded_headers(name_data=\"dblp_journal_published_in\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f3fa24e-4dc7-49cf-bad5-fad30d36531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = [article, book, data, incollection, inproceedings, mastersthesis, phdthesis, proceedings, www]\n",
    "name_tables = [\"article\", \"book\", \"data\", \"incollection\", \"inproceedings\", \"mastersthesis\", \"phdthesis\", \"proceedings\", \"www\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06468ef8",
   "metadata": {},
   "source": [
    "#### Creation of authors data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15aa01e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_authors_relation = article.select(F.explode(F.split(\"author\", \"\\|\")).alias(\"author_name\"), col(\"article\").alias(\"article_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfe495f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_authors_relation = article_authors_relation.withColumn(\"author_id\", F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66ad190b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "authors = article_authors_relation.select(col(\"author_name\").alias(\"name\"),\"author_id\")\n",
    "authors_data = create_fake_email(df=authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9483a3",
   "metadata": {},
   "source": [
    "#### Affiliation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cf565e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "schools = phdthesis.select(\"school\").dropDuplicates()\n",
    "schools = schools.withColumn(\"number\", F.floor(F.rand()*20)).dropDuplicates(['number'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cb6076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "affiliation_data = authors_data.select(\"author_id\", F.floor(F.rand()*20).alias(\"number\")).join(schools, on=\"number\", how=\"left\").drop(\"number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53b0efb",
   "metadata": {},
   "source": [
    "#### Creation of article data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5b359e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data = article.select(\n",
    "    col(\"article\").alias(\"id\"),\n",
    "    col(\"author\").alias(\"author_name\"),\n",
    "    \"journal\",\n",
    "    \"title\", \n",
    "    \"url\",\n",
    "    \"volume\",\n",
    "    \"year\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020ae30d",
   "metadata": {},
   "source": [
    "#### Create citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad3c857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations = article_data.select(\"id\",\"year\").dropDuplicates()\n",
    "\n",
    "citationsA = citations.withColumn(\"number\",  F.round(F.exp(F.rand()),3)).persist(StorageLevel.DISK_ONLY)\n",
    "citationsB = citations.select(col(\"id\").alias(\"id_cited\"), col(\"year\").alias(\"year_cited\")).withColumn(\"number\",  F.round(F.exp(F.rand()),3)).persist(StorageLevel.DISK_ONLY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4249e50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = (citationsB['year_cited'] >= citationsA['year']) & (citationsB['number'] == citationsA['number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07720d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations_data = citationsA.join(citationsB,on=cond).drop(\"number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27609cc0",
   "metadata": {},
   "source": [
    "#### Creation of journal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8563223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_data = article.select(\n",
    "    \"journal\"\n",
    ").dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2024d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window().orderBy(\"n_articles\")\n",
    "journal_rank = (\n",
    "    article\n",
    "    .groupBy(\"journal\")\n",
    "    .agg(F.countDistinct(\"article\").alias(\"n_articles\"))\n",
    "    .orderBy(col(\"n_articles\").desc())\n",
    "    .limit(200)\n",
    "    .withColumn(\"rank\", F.row_number().over(w))\n",
    "    .select(\"rank\", \"journal\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a183c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_data_w_rank = journal_data.join(journal_rank, on=\"journal\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cd6508",
   "metadata": {},
   "source": [
    "#### Creation of time data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8d87adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = article.select(\"year\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7142e8",
   "metadata": {},
   "source": [
    "### Creation of conferences information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66ad190b",
   "metadata": {},
   "outputs": [],
   "source": [
    "proceedings_ = (\n",
    "    proceedings\n",
    "    .withColumn(\"type\", F.split(\"url\", \"\\/\")[1])\n",
    "    .withColumn(\"conference_name\", F.split(\"url\", \"\\/\")[2])\n",
    "    .withColumn(\"edition\", F.concat_ws(\"-\",\"conference_name\", \"year\"))\n",
    "    .filter(col(\"type\") == 'conf')\n",
    "    .select(\"type\",\"conference_name\",\"edition\", \"editor\", \"year\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0f67986",
   "metadata": {},
   "outputs": [],
   "source": [
    "inproceedings_ = (\n",
    "    inproceedings\n",
    "    .withColumn(\"type\", F.split(\"url\", \"\\/\")[1])\n",
    "    .withColumn(\"conference_name\", F.split(\"url\", \"\\/\")[2])\n",
    "    .withColumn(\"edition\", F.concat_ws(\"-\",\"conference_name\", \"year\"))\n",
    "    .filter(col(\"type\") == 'conf')\n",
    "    .select(\"type\",\"conference_name\",\"edition\", \"editor\", \"year\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "672aa58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conference_information = proceedings_.union(inproceedings_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25a92ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:===================>                                     (6 + 12) / 18]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "conference_information.write.mode(\"overwrite\").parquet(\"checkpoint\")\n",
    "conference_information = spark.read.parquet(\"checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25a92ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = [ \n",
    "    \"New York City, USA\", \"London, UK\", \"Tokyo, Japan\", \"Paris, France\", \"Los Angeles, USA\", \"Beijing, China\", \"Moscow, Russia\", \"Istanbul, Turkey\", \"Sao Paulo, Brazil\",\n",
    "    \"Cairo, Egypt\", \"Mumbai, India\", \"Mexico City, Mexico\", \"Seoul, South Korea\", \"Jakarta, Indonesia\", \"Karachi, Pakistan\", \"Buenos Aires, Argentina\", \"Delhi, India\", \n",
    "    \"Shanghai, China\", \"Manila, Philippines\", \"Dhaka, Bangladesh\", \"Moscow, Russia\", \"Istanbul, Turkey\", \"Tianjin, China\", \"Rio de Janeiro, Brazil\", \"Lagos, Nigeria\", \"Lima, Peru\", \n",
    "    \"Bangkok, Thailand\", \"Jakarta, Indonesia\", \"Cairo, Egypt\", \"Bogota, Colombia\", \"Kinshasa, Democratic Republic of the Congo\", \"Seoul, South Korea\", \"Dhaka, Bangladesh\", \"Karachi, Pakistan\", \n",
    "    \"Tokyo, Japan\", \"Manila, Philippines\", \"Guangzhou, China\", \"Mumbai, India\", \"Istanbul, Turkey\", \"Moscow, Russia\", \"Sao Paulo, Brazil\", \"Beijing, China\", \"Lahore, Pakistan\", \n",
    "    \"Shenzhen, China\", \"Chongqing, China\", \"Chengdu, China\", \"Lahore, Pakistan\", \"Kinshasa, Democratic Republic of the Congo\", \"Bangalore, India\", \"Taipei, Taiwan\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f088809",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.orderBy(\"city\")\n",
    "cities = spark.createDataFrame([(city,) for city in cities], [\"city\"]).withColumn(\"number\", F.row_number().over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3288611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conference_information_w_random_city = (\n",
    "    conference_information\n",
    "    .withColumn(\"number\", F.floor(F.rand()*50))\n",
    "    .join(F.broadcast(cities), on=\"number\", how=\"left\")\n",
    "    .dropDuplicates(['edition'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5bde04",
   "metadata": {},
   "source": [
    "#### Create random link between article data and conference edition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45c5f5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_info_random = conference_information_w_random_city.select(\"edition\",\"year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "160f38fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=============================================>             (7 + 2) / 9]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "conf_info_random.write.mode(\"overwrite\").parquet(\"checkpoint2\")\n",
    "conf_info_random = spark.read.parquet(\"checkpoint2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78d0fc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy(\"id\").orderBy(F.rand())\n",
    "\n",
    "\n",
    "\n",
    "article_data_w_conf_info = (\n",
    "    article_data\n",
    "    .join(F.broadcast(conf_info_random), on=[\"year\"], how=\"left\")\n",
    "    .withColumn(\"row\", F.row_number().over(w))\n",
    "    .filter(col(\"row\")==1).drop(\"row\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a72a18db",
   "metadata": {},
   "outputs": [],
   "source": [
    "conference_information_w_random_city = conference_information_w_random_city.join(article_data_w_conf_info.select(\"edition\"), on=\"edition\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca3edb6",
   "metadata": {},
   "source": [
    "#### Create conference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5deb6483",
   "metadata": {},
   "outputs": [],
   "source": [
    "conference_data = conference_information_w_random_city.select(\"conference_name\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7395d8c",
   "metadata": {},
   "source": [
    "#### Create edition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "39e45bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "edition_data = conference_information_w_random_city.select(\"conference_name\", col(\"edition\").alias(\"conference_edition\"), \"city\", \"year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf06ec2",
   "metadata": {},
   "source": [
    "#### Create city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "49c0a3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_data = cities.drop(\"number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf216cd",
   "metadata": {},
   "source": [
    "#### Create random link between reviewers and articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d5f32ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = authors_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a46bea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.orderBy(F.rand())\n",
    "\n",
    "authors_rand = authors_data.withColumn(\"number\", F.row_number().over(w)).select(\"number\",\"author_id\").dropDuplicates(['number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7290b1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/10 21:23:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/10 21:23:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/10 21:23:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/10 21:23:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/10 21:23:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/10 21:23:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "relation1 = (\n",
    "    article_data\n",
    "    .select(\"id\")\n",
    "    .withColumn(\"number\", F.floor(F.rand()*num))\n",
    "    .join(authors_rand, on=\"number\", how=\"left\")\n",
    "    .withColumn(\"approved\", when(F.rand()*4 > .5, \"Yes\").otherwise(\"No\"))\n",
    ").persist(StorageLevel.DISK_ONLY)\n",
    "relation2 = (\n",
    "    article_data\n",
    "    .select(\"id\")\n",
    "    .withColumn(\"number\", F.floor(F.rand()*num))\n",
    "    .join(authors_rand, on=\"number\", how=\"left\")\n",
    "    .withColumn(\"approved\", when(F.rand()*4 > .5, \"Yes\").otherwise(\"No\"))\n",
    ").persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "relation3 = (\n",
    "    article_data\n",
    "    .select(\"id\")\n",
    "    .withColumn(\"number\", F.floor(F.rand()*num))\n",
    "    .join(authors_rand, on=\"number\", how=\"left\")\n",
    "    .withColumn(\"approved\", when(F.rand()*4 > .5, \"Yes\").otherwise(\"No\"))\n",
    ").persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "\n",
    "relation_author_reviewer = relation1.union(relation2).union(relation3).drop(\"number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17642e03",
   "metadata": {},
   "source": [
    "#### Communities data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ffbe1b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = ['Big Data', \"AI\", \"Data Sciene\", \"Hardware\", \"Computer Systems\", \"Database\", \"Web\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "545787c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "categories = ['Big Data', 'AI', 'Data Science', 'Hardware', 'Computer Systems', 'Web']\n",
    "num_categories = len(categories)\n",
    "\n",
    "\n",
    "# Generate random numbers\n",
    "df_with_random = article_data.select(\"id\").withColumn(\"random\", F.rand())\n",
    "\n",
    "# Define buckets for discretization\n",
    "splits = [float(i) / num_categories for i in range(num_categories + 1)]\n",
    "\n",
    "# Define bucketizer\n",
    "bucketizer = Bucketizer(splits=splits, inputCol=\"random\", outputCol=\"categoryIndex\")\n",
    "\n",
    "# Apply bucketizer to assign category indices\n",
    "df_with_category_index = bucketizer.transform(df_with_random)\n",
    "\n",
    "# Assign category names based on category indices\n",
    "df_with_category = df_with_category_index.withColumn(\n",
    "    \"category\", \n",
    "    when(F.rand() <.9, \"database\")\n",
    "    .when(col(\"categoryIndex\") == 0, categories[0]) \\\n",
    "    .when(col(\"categoryIndex\") == 1, categories[1]) \\\n",
    "    .when(col(\"categoryIndex\") == 2, categories[2]) \\\n",
    "    .when(col(\"categoryIndex\") == 3, categories[3]) \\\n",
    "    .when(col(\"categoryIndex\") == 4, categories[4]) \\\n",
    "    .otherwise(categories[5])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7ee955e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities_data = df_with_category.select(\"id\",\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9142e5",
   "metadata": {},
   "source": [
    "### Persist to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "162b9b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/10 21:23:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/10 21:23:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/10 21:23:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/10 21:23:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/10 21:23:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/10 21:23:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/10 21:23:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/10 21:23:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/10 21:23:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/10 21:24:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/10 21:24:03 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/10 21:24:04 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/10 21:24:04 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/10 21:24:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/10 21:24:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/10 21:24:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/10 21:24:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/10 21:24:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/10 21:24:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/10 21:24:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/10 21:24:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/10 21:24:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/10 21:24:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/10 21:24:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/10 21:24:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "authors_data.write.mode(\"overwrite\").parquet(f\"../temporal_zone/authors_data\")\n",
    "article_data_w_conf_info.write.mode(\"overwrite\").parquet(f\"../temporal_zone/article_data\")\n",
    "article_authors_relation.write.mode(\"overwrite\").parquet(f\"../temporal_zone/article_authors_relation\")\n",
    "journal_data_w_rank.write.mode(\"overwrite\").parquet(f\"../temporal_zone/journal_data_w_rank\")\n",
    "time.write.mode(\"overwrite\").parquet(f\"../temporal_zone/time\")\n",
    "conference_data.write.mode(\"overwrite\").parquet(f\"../temporal_zone/conference_data\")\n",
    "edition_data.write.mode(\"overwrite\").parquet(f\"../temporal_zone/edition_data\")\n",
    "city_data.write.mode(\"overwrite\").parquet(f\"../temporal_zone/city_data\")\n",
    "relation_author_reviewer.write.mode(\"overwrite\").parquet(f\"../temporal_zone/reviews_data\")\n",
    "citations_data.write.mode(\"overwrite\").parquet(f\"../temporal_zone/citations_data\")\n",
    "communities_data.write.mode(\"overwrite\").parquet(f\"../temporal_zone/communities_data\")\n",
    "affiliation_data.write.mode(\"overwrite\").parquet(f\"../temporal_zone/affiliation_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b724c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def delete_quotes(df):\n",
    "    # List to store expressions for select\n",
    "    exprs = []\n",
    "    \n",
    "    # Iterate through all columns\n",
    "    for column in df.columns:\n",
    "        # Check if the column is of StringType\n",
    "        if df.schema[column].dataType == StringType():\n",
    "            # Replace quotes with empty string in the column\n",
    "            exprs.append(F.regexp_replace(col(column), '\"', '').alias(column))\n",
    "        else:\n",
    "            exprs.append(col(column))\n",
    "    \n",
    "    # Select the modified columns\n",
    "    return df.select(*exprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e6a6f8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 36672)\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/root/miniconda3/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/root/miniconda3/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/root/miniconda3/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/root/miniconda3/lib/python3.11/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/root/miniconda3/lib/python3.11/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.11/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.11/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Define a list of file paths\n",
    "file_paths = [\n",
    "    \"../temporal_zone/authors_data\",\n",
    "    \"../temporal_zone/article_data\",\n",
    "    \"../temporal_zone/article_authors_relation\",\n",
    "    \"../temporal_zone/journal_data_w_rank\",\n",
    "    \"../temporal_zone/time\",\n",
    "    \"../temporal_zone/conference_data\",\n",
    "    \"../temporal_zone/edition_data\",\n",
    "    \"../temporal_zone/city_data\",\n",
    "    \"../temporal_zone/reviews_data\",\n",
    "    \"../temporal_zone/citations_data\",\n",
    "    \"../temporal_zone/communities_data\",\n",
    "    \"../temporal_zone/affiliation_data\"\n",
    "\n",
    "]\n",
    "\n",
    "# Loop through each file path\n",
    "for file_path in file_paths:\n",
    "    # Write the DataFrame to CSV\n",
    "    spark.read.parquet(file_path).transform(delete_quotes).coalesce(1).write.mode(\"overwrite\").options(header=\"true\").csv(f\"{file_path}_tmp\")\n",
    "\n",
    "    # Get the path of the single CSV file\n",
    "    csv_file_path = f\"{file_path}_tmp/*.csv\"\n",
    "    files = os.listdir(f\"{file_path}_tmp\")\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            csv_file_path = os.path.join(f\"{file_path}_tmp\", file)\n",
    "            break\n",
    "\n",
    "    # Rename the CSV file\n",
    "    os.rename(csv_file_path, f\"{file_path}.csv\")\n",
    "\n",
    "    # Delete the temporary folder\n",
    "    shutil.rmtree(f\"{file_path}_tmp\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
